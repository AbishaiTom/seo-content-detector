{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aebd003def0d48d2af5bb4e08b6dd5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb6e3ba2525d45be897b9fbd90881d53",
              "IPY_MODEL_fba1d67373384a8581c77d10d5a1675c",
              "IPY_MODEL_f0cf771bb4e54fc1a8122b01331ce7fd"
            ],
            "layout": "IPY_MODEL_3f8fb9e912dc4faba4285bb4e24ea6f5"
          }
        },
        "fb6e3ba2525d45be897b9fbd90881d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f08adef0c141e4af6a86a84de2f19c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ff97697eee2d4c96ad5dbd8bc95309b6",
            "value": "Batches:â€‡100%"
          }
        },
        "fba1d67373384a8581c77d10d5a1675c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9cab209125742de9afbea72c0d2b3f6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0bbcc9431ba485292624486ba6c231a",
            "value": 3
          }
        },
        "f0cf771bb4e54fc1a8122b01331ce7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6411a75e099d45ff9614e72b558a3b28",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_495d146edefb42af94f900d0ed60cfff",
            "value": "â€‡3/3â€‡[00:01&lt;00:00,â€‡â€‡1.26it/s]"
          }
        },
        "3f8fb9e912dc4faba4285bb4e24ea6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f08adef0c141e4af6a86a84de2f19c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff97697eee2d4c96ad5dbd8bc95309b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9cab209125742de9afbea72c0d2b3f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0bbcc9431ba485292624486ba6c231a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6411a75e099d45ff9614e72b558a3b28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "495d146edefb42af94f900d0ed60cfff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0stDOTNUNV5u",
        "outputId": "cc625ecb-cc3f-4980-8ec6-154a0720bf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.12/dist-packages (0.7.10)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.12/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# Install all the libraries needed for the whole project\n",
        "!pip install pandas beautifulsoup4 textstat scikit-learn sentence-transformers nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SEO Content Quality & Duplicate Detection - HTML Parsing Step\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#1. Define paths\n",
        "\n",
        "input_path = \"data/data.csv\"\n",
        "output_path = \"data/extracted_content.csv\"\n",
        "\n",
        "# Make sure the 'data' directory exists\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# === 2. Load dataset ========================================================\n",
        "print(f\"Reading dataset from: {input_path}\\n\")\n",
        "\n",
        "# Try to load the file, give a clear error if it's missing\n",
        "try:\n",
        "    df = pd.read_csv(input_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"--- ERROR! ---\")\n",
        "    print(f\"Could not find {input_path}.\")\n",
        "    print(\"Please create a 'data' folder in the Colab file browser and upload your 'data.csv' file.\")\n",
        "    # Stop the script if the file isn't there\n",
        "    raise\n",
        "\n",
        "print(f\"Loaded {len(df)} records.\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# === 3. HTML parsing function ==============================================\n",
        "\n",
        "def parse_html(html_string):\n",
        "    \"\"\"\n",
        "    Parse HTML content to extract title, body text, and word count.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Not all 'html_content' cells might be valid strings, check for NaNs\n",
        "        if not isinstance(html_string, str):\n",
        "            return {\"title\": None, \"body_text\": None, \"word_count\": 0}\n",
        "\n",
        "        soup = BeautifulSoup(html_string, \"html.parser\")\n",
        "\n",
        "        # Extract <title> tag text\n",
        "        title_tag = soup.find(\"title\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else None\n",
        "\n",
        "        # Extract main content: Try <article>, then <main>, then all <p> tags\n",
        "        # This fallback logic is key for handling different site structures.\n",
        "        body_text = \"\"\n",
        "        if soup.find(\"article\"):\n",
        "            body_text = soup.find(\"article\").get_text(separator=\" \", strip=True)\n",
        "        elif soup.find(\"main\"):\n",
        "            body_text = soup.find(\"main\").get_text(separator=\" \", strip=True)\n",
        "        else:\n",
        "            # Fallback: just stitch all paragraphs together\n",
        "            paragraphs = soup.find_all(\"p\")\n",
        "            body_text = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
        "\n",
        "        # Clean up extra whitespace (newlines, tabs, multiple spaces)\n",
        "        body_text = re.sub(r\"\\s+\", \" \", body_text).strip()\n",
        "\n",
        "        # Word count\n",
        "        word_count = len(body_text.split()) if body_text else 0\n",
        "\n",
        "        return {\"title\": title, \"body_text\": body_text, \"word_count\": word_count}\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other weird parsing errors\n",
        "        print(f\"âš ï¸ Error parsing HTML: {str(e)[:100]}\")\n",
        "        return {\"title\": None, \"body_text\": None, \"word_count\": 0}\n",
        "\n",
        "# === 4. Apply parsing ======================================================\n",
        "\n",
        "print(\"\\nðŸ” Parsing HTML content... (this might take a minute)\")\n",
        "\n",
        "# Run the function on every row in the 'html_content' column\n",
        "parsed_results = df[\"html_content\"].apply(parse_html)\n",
        "\n",
        "# This is a clean way to turn a Series of dicts into a DataFrame\n",
        "parsed_df = parsed_results.apply(pd.Series)\n",
        "\n",
        "# Combine our new columns (title, etc.) with the original 'url'\n",
        "extracted_df = pd.concat([df[\"url\"], parsed_df], axis=1)\n",
        "\n",
        "# === 5. Stats & Save =======================================================\n",
        "\n",
        "print(\"\\nðŸ“Š Parsing Statistics:\")\n",
        "# Filter out any rows where we failed to get text\n",
        "valid_rows = extracted_df[\"body_text\"].notna() & (extracted_df[\"word_count\"] > 0)\n",
        "num_valid = valid_rows.sum()\n",
        "\n",
        "print(f\"Successfully parsed: {num_valid} / {len(extracted_df)}\")\n",
        "print(f\"Average word count (of valid pages): {extracted_df.loc[valid_rows, 'word_count'].mean():.1f}\")\n",
        "print(f\"Pages with <500 words (thin content): {(extracted_df['word_count'] < 500).sum()}\")\n",
        "\n",
        "# Drop failed/empty rows before saving\n",
        "extracted_df = extracted_df[valid_rows].copy()\n",
        "\n",
        "# Save to CSV. Using utf-8-sig avoids potential encoding issues in Excel\n",
        "extracted_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"\\n Extracted content saved to: {output_path}\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\n Sample of extracted content:\")\n",
        "print(extracted_df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMNi1wiTXYHh",
        "outputId": "8eea6a2d-1d7a-4405-e974-d1fceac75d18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading dataset from: data/data.csv\n",
            "\n",
            "Loaded 81 records.\n",
            "Columns: ['url', 'html_content']\n",
            "\n",
            "ðŸ” Parsing HTML content... (this might take a minute)\n",
            "\n",
            "ðŸ“Š Parsing Statistics:\n",
            "Successfully parsed: 68 / 81\n",
            "Average word count (of valid pages): 3290.9\n",
            "Pages with <500 words (thin content): 36\n",
            "\n",
            " Extracted content saved to: data/extracted_content.csv\n",
            "\n",
            " Sample of extracted content:\n",
            "                                                 url  \\\n",
            "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
            "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
            "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
            "\n",
            "                                               title  \\\n",
            "0                                Cyber Security Blog   \n",
            "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
            "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
            "\n",
            "                                           body_text  word_count  \n",
            "0  Cyber Crisis Tabletop Exercise Cyber Security ...       325.0  \n",
            "1  Cybersecurity is gaining more importance globa...      1700.0  \n",
            "2  Home Insights Blog Posts 11 Cyber Defense Tips...      1058.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Phase 2: Text Preprocessing & Feature Engineering\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "import textstat\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# === 1. Download NLTK data ==================================================\n",
        "# We need the 'punkt' tokenizer for splitting text into sentences\n",
        "nltk.download('punkt')\n",
        "# We also need stopwords to filter them out for keyword extraction\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# === 2. Load extracted data ================================================\n",
        "print(\"Loading extracted_content.csv...\")\n",
        "features_df = pd.read_csv(\"data/extracted_content.csv\")\n",
        "\n",
        "# === 3. Define Feature Engineering Functions ==============================\n",
        "\n",
        "def get_sentence_count(text):\n",
        "    \"\"\"Calculates the total number of sentences in a text.\"\"\"\n",
        "    try:\n",
        "        return len(nltk.sent_tokenize(text))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def get_readability_score(text):\n",
        "    \"\"\"Calculates the Flesch Reading Ease score.\"\"\"\n",
        "    try:\n",
        "        return textstat.flesch_reading_ease(text)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Let's get our text corpus ready\n",
        "corpus = features_df['body_text'].tolist()\n",
        "\n",
        "# === 4. Calculate Basic & Readability Features ==========================\n",
        "print(\"Calculating basic text features (sentences, readability)...\")\n",
        "\n",
        "features_df['sentence_count'] = features_df['body_text'].apply(get_sentence_count)\n",
        "features_df['flesch_reading_ease'] = features_df['body_text'].apply(get_readability_score)\n",
        "\n",
        "# === 5. Extract Keywords (TF-IDF) =======================================\n",
        "print(\"Extracting keywords using TF-IDF...\")\n",
        "\n",
        "# We'll use TF-IDF to find the top keywords.\n",
        "# min_df=2: Ignore words that appear in only 1 document\n",
        "# max_df=0.8: Ignore words that appear in > 80% of documents (too common)\n",
        "tfidf_vec = TfidfVectorizer(stop_words='english', max_features=1000, min_df=2, max_df=0.8)\n",
        "tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
        "\n",
        "# Get the actual feature names (the words)\n",
        "feature_names = tfidf_vec.get_feature_names_out()\n",
        "\n",
        "def get_top_keywords(tfidf_row, feature_names, top_n=5):\n",
        "    \"\"\"Gets the top_n keywords from a single document's TF-IDF vector.\"\"\"\n",
        "    # Get the indices of the top_n scores\n",
        "    top_indices = tfidf_row.argsort()[-top_n:][::-1]\n",
        "    # Get the words and scores\n",
        "    keywords = [feature_names[i] for i in top_indices]\n",
        "    return ', '.join(keywords) # Store as a simple comma-separated string\n",
        "\n",
        "# Get the dense version of the row from the sparse matrix\n",
        "top_keywords_list = [get_top_keywords(tfidf_matrix[i].toarray().flatten(), feature_names) for i in range(len(corpus))]\n",
        "features_df['top_keywords'] = top_keywords_list\n",
        "\n",
        "# === 6. Generate Sentence Embeddings ====================================\n",
        "print(\"Generating sentence embeddings... (This may take a moment)\")\n",
        "\n",
        "# 'all-MiniLM-L6-v2' is a fantastic model: it's fast, small, and great\n",
        "# for tasks like similarity and clustering.\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# This encodes all 68 text bodies into a 384-dimension vector\n",
        "# We do this in one batch, which is much faster.\n",
        "embeddings = embedding_model.encode(corpus, show_progress_bar=True)\n",
        "\n",
        "print(f\"Embeddings generated with shape: {embeddings.shape}\")\n",
        "\n",
        "# === 7. Save Features & Embeddings ======================================\n",
        "\n",
        "# We'll save the text features to 'features.csv'\n",
        "# It's bad practice to save a giant array (the embedding) in a CSV cell.\n",
        "# The professional way is to save the embeddings in a separate, optimized file.\n",
        "# We'll use NumPy's .npy format for this.\n",
        "\n",
        "# Let's clean up the df before saving\n",
        "features_to_save = features_df[['url', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords']]\n",
        "\n",
        "features_to_save.to_csv('data/features.csv', index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n Text features saved to data/features.csv\")\n",
        "\n",
        "# Save the embeddings array\n",
        "np.save('data/embeddings.npy', embeddings)\n",
        "print(f\" Embeddings saved to data/embeddings.npy\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\nðŸ”¹ Sample of features:\")\n",
        "print(features_to_save.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "aebd003def0d48d2af5bb4e08b6dd5cf",
            "fb6e3ba2525d45be897b9fbd90881d53",
            "fba1d67373384a8581c77d10d5a1675c",
            "f0cf771bb4e54fc1a8122b01331ce7fd",
            "3f8fb9e912dc4faba4285bb4e24ea6f5",
            "b2f08adef0c141e4af6a86a84de2f19c",
            "ff97697eee2d4c96ad5dbd8bc95309b6",
            "c9cab209125742de9afbea72c0d2b3f6",
            "f0bbcc9431ba485292624486ba6c231a",
            "6411a75e099d45ff9614e72b558a3b28",
            "495d146edefb42af94f900d0ed60cfff"
          ]
        },
        "id": "kuwlbwQGXx1Y",
        "outputId": "1a1b6532-ff4f-48e1-f71d-960ddc6ea9fb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading extracted_content.csv...\n",
            "Calculating basic text features (sentences, readability)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting keywords using TF-IDF...\n",
            "Generating sentence embeddings... (This may take a moment)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aebd003def0d48d2af5bb4e08b6dd5cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generated with shape: (68, 384)\n",
            "\n",
            " Text features saved to data/features.csv\n",
            " Embeddings saved to data/embeddings.npy\n",
            "\n",
            "ðŸ”¹ Sample of features:\n",
            "                                                 url  word_count  \\\n",
            "0     https://www.cm-alliance.com/cybersecurity-blog       325.0   \n",
            "1    https://www.varonis.com/blog/cybersecurity-tips      1700.0   \n",
            "2  https://www.cisecurity.org/insights/blog/11-cy...      1058.0   \n",
            "\n",
            "   sentence_count  flesch_reading_ease  \\\n",
            "0               5            -6.967854   \n",
            "1              92            41.465000   \n",
            "2              73            53.262918   \n",
            "\n",
            "                                        top_keywords  \n",
            "0  cyber, cybersecurity, training, management, se...  \n",
            "1                   data, access, security, app, mfa  \n",
            "2          authentication, don, cyber, home, protect  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Phase 3: Duplicate Content Detection\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# === 1. Load Features and Embeddings =====================================\n",
        "print(\"Loading features and embeddings...\")\n",
        "try:\n",
        "    features_df = pd.read_csv('data/features.csv')\n",
        "    embeddings = np.load('data/embeddings.npy')\n",
        "    urls = features_df['url'].tolist() # Get a list of URLs for reference\n",
        "\n",
        "    # Check that our data matches\n",
        "    if len(features_df) != embeddings.shape[0]:\n",
        "        print(f\"--- ERROR! ---\")\n",
        "        print(f\"Mismatch! features.csv has {len(features_df)} rows but embeddings.npy has {embeddings.shape[0]} rows.\")\n",
        "        raise\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"--- ERROR! ---\")\n",
        "    print(\"Could not find features.csv or embeddings.npy. Did Phase 2 run correctly?\")\n",
        "    raise\n",
        "\n",
        "# === 2. Calculate Cosine Similarity ======================================\n",
        "print(\"Calculating cosine similarity matrix...\")\n",
        "# This computes the similarity score between every document and every other document\n",
        "# The result is a 68x68 matrix (in your case)\n",
        "cosine_sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# === 3. Find Duplicate Pairs =============================================\n",
        "print(\"Finding duplicate pairs...\")\n",
        "# Per the assignment, we'll use a threshold of 0.80\n",
        "SIMILARITY_THRESHOLD = 0.80\n",
        "duplicates_found = []\n",
        "\n",
        "# We iterate through the *upper triangle* of the matrix\n",
        "# This stops us from checking a doc against itself (i==j)\n",
        "# and from checking pairs twice (e.g., A-B and B-A)\n",
        "for i in range(len(cosine_sim_matrix)):\n",
        "    for j in range(i + 1, len(cosine_sim_matrix)):\n",
        "        similarity = cosine_sim_matrix[i][j]\n",
        "\n",
        "        if similarity > SIMILARITY_THRESHOLD:\n",
        "            # We found a pair!\n",
        "            duplicates_found.append({\n",
        "                'url_a': urls[i],\n",
        "                'url_b': urls[j],\n",
        "                'similarity_score': similarity\n",
        "            })\n",
        "\n",
        "# === 4. Save Duplicates ==================================================\n",
        "df_duplicates = pd.DataFrame(duplicates_found)\n",
        "df_duplicates.to_csv('data/duplicates.csv', index=False, encoding='utf-8-sig')\n",
        "print(f\" Found {len(df_duplicates)} duplicate pairs. Saved to data/duplicates.csv\")\n",
        "\n",
        "# === 5. Print Final Analysis Summary =====================================\n",
        "# This part answers the prompt's analysis questions directly\n",
        "thin_content_count = (features_df['word_count'] < 500).sum()\n",
        "total_pages = len(features_df)\n",
        "\n",
        "print(\"\\n--- SEO Content Analysis Summary ---\")\n",
        "print(f\"Total Pages Analyzed: {total_pages}\")\n",
        "print(f\"Duplicate Pairs Found (>{SIMILARITY_THRESHOLD*100}% similarity): {len(df_duplicates)}\")\n",
        "print(f\"Thin Content Pages (<500 words): {thin_content_count} ({(thin_content_count/total_pages)*100:.1f}%)\")\n",
        "\n",
        "if len(df_duplicates) > 0:\n",
        "    print(\"\\nðŸ”¹ Sample of duplicate pairs:\")\n",
        "    print(df_duplicates.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHwKF34vYpzz",
        "outputId": "34a64e4c-f01d-4b76-9122-0c4236e64d05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features and embeddings...\n",
            "Calculating cosine similarity matrix...\n",
            "Finding duplicate pairs...\n",
            " Found 10 duplicate pairs. Saved to data/duplicates.csv\n",
            "\n",
            "--- SEO Content Analysis Summary ---\n",
            "Total Pages Analyzed: 68\n",
            "Duplicate Pairs Found (>80.0% similarity): 10\n",
            "Thin Content Pages (<500 words): 23 (33.8%)\n",
            "\n",
            "ðŸ”¹ Sample of duplicate pairs:\n",
            "                                               url_a  \\\n",
            "0  https://en.wikipedia.org/wiki/Search_engine_op...   \n",
            "1  https://en.wikipedia.org/wiki/Search_engine_op...   \n",
            "2  https://en.wikipedia.org/wiki/Search_engine_op...   \n",
            "3  https://simple.wikipedia.org/wiki/Search_engin...   \n",
            "4  https://simple.wikipedia.org/wiki/Search_engin...   \n",
            "\n",
            "                                               url_b  similarity_score  \n",
            "0  https://simple.wikipedia.org/wiki/Search_engin...          0.995893  \n",
            "1     https://en.wikipedia.org/wiki/Machine_learning          0.805831  \n",
            "2  https://simple.wikipedia.org/wiki/Machine_lear...          0.808370  \n",
            "3     https://en.wikipedia.org/wiki/Machine_learning          0.820781  \n",
            "4  https://simple.wikipedia.org/wiki/Machine_lear...          0.821608  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Phase 4: Content Quality Model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib # For saving our trained model\n",
        "\n",
        "# === 1. Load Features ======================================================\n",
        "print(\"Loading features.csv...\")\n",
        "model_df = pd.read_csv('data/features.csv')\n",
        "\n",
        "# === 2. Create Quality Labels (Target Variable) ==========================\n",
        "# We're applying the business rules from the assignment doc\n",
        "# to create our 'y' variable (the thing we want to predict).\n",
        "\n",
        "# Define the conditions\n",
        "conditions = [\n",
        "    # High Quality\n",
        "    (model_df['word_count'] > 1500) & (model_df['flesch_reading_ease'] >= 50) & (model_df['flesch_reading_ease'] <= 70),\n",
        "    # Low Quality\n",
        "    (model_df['word_count'] < 500) | (model_df['flesch_reading_ease'] < 30)\n",
        "]\n",
        "\n",
        "# Define the corresponding labels\n",
        "labels = ['High', 'Low']\n",
        "\n",
        "# np.select is a clean way to do this. 'Medium' is the default for anything\n",
        "# that doesn't meet the High or Low criteria.\n",
        "model_df['quality_label'] = np.select(conditions, labels, default='Medium')\n",
        "\n",
        "print(\"Generated quality labels:\")\n",
        "print(model_df['quality_label'].value_counts())\n",
        "\n",
        "# === 3. Prepare Data for Modeling ========================================\n",
        "\n",
        "# Define our features (X) and our target (y)\n",
        "# We'll use these features to PREDICT the quality_label\n",
        "feature_columns = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
        "X = model_df[feature_columns]\n",
        "y = model_df['quality_label']\n",
        "\n",
        "# Split our data: 70% for training, 30% for testing\n",
        "# random_state=42 ensures we get the same split every time we run\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nData split: {len(X_train)} training samples, {len(X_test)} testing samples.\")\n",
        "\n",
        "# ---\n",
        "# NOTE: With only 68 total samples, our test set is tiny (~21 samples).\n",
        "# Model performance might not be stable, but we follow the process.\n",
        "# ---\n",
        "\n",
        "# === 4. Train a Model ====================================================\n",
        "print(\"Training Logistic Regression model...\")\n",
        "\n",
        "# We'll use LogisticRegression. It's simple, fast, and easy to interpret.\n",
        "# `class_weight='balanced'` helps the model handle cases where we have\n",
        "# way more 'Low' than 'High' labels.\n",
        "model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\" Model trained.\")\n",
        "\n",
        "# === 5. Evaluate the Model ===============================================\n",
        "print(\"Evaluating model performance...\")\n",
        "\n",
        "# Get predictions on the unseen test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the results!\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(\"Rows = Actual, Columns = Predicted\")\n",
        "print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
        "                 index=model.classes_ + \" (Actual)\",\n",
        "                 columns=model.classes_ + \" (Predicted)\"))\n",
        "\n",
        "# === 6. Save the Model ===================================================\n",
        "# We save the trained model so we can use it later (in our demo)\n",
        "# without having to retrain it every time.\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_path = 'models/quality_model.joblib'\n",
        "joblib.dump(model, model_path)\n",
        "\n",
        "print(f\"\\n Model saved to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4I-FiEeZkLM",
        "outputId": "8557f422-342e-47f9-a878-1283976b90dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features.csv...\n",
            "Generated quality labels:\n",
            "quality_label\n",
            "Low       35\n",
            "Medium    25\n",
            "High       8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Data split: 47 training samples, 21 testing samples.\n",
            "Training Logistic Regression model...\n",
            " Model trained.\n",
            "Evaluating model performance...\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        High       0.00      0.00      0.00         2\n",
            "         Low       0.82      0.82      0.82        11\n",
            "      Medium       0.43      0.38      0.40         8\n",
            "\n",
            "    accuracy                           0.57        21\n",
            "   macro avg       0.42      0.40      0.41        21\n",
            "weighted avg       0.59      0.57      0.58        21\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "Rows = Actual, Columns = Predicted\n",
            "                 High (Predicted)  Low (Predicted)  Medium (Predicted)\n",
            "High (Actual)                   0                0                   2\n",
            "Low (Actual)                    0                9                   2\n",
            "Medium (Actual)                 3                2                   3\n",
            "\n",
            " Model saved to models/quality_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Phase 5: Real-Time Analysis Function (Demo)\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# We need to load our saved model\n",
        "try:\n",
        "    demo_model = joblib.load('models/quality_model.joblib')\n",
        "    print(\"Loaded trained model 'models/quality_model.joblib'\")\n",
        "except FileNotFoundError:\n",
        "    print(\"--- ERROR! ---\")\n",
        "    print(\"Model file not found. Did Phase 4 run correctly?\")\n",
        "    raise\n",
        "\n",
        "# We also need the parsing function from Phase 1.\n",
        "# We can just re-run the cell from Phase 1, or copy it here.\n",
        "# For a clean notebook, just ensure Phase 1's cell has been run.\n",
        "# (We'll assume 'parse_html' is already in memory)\n",
        "\n",
        "def analyze_url(url):\n",
        "    \"\"\"\n",
        "    Takes a single URL, scrapes it, analyzes it, and returns\n",
        "    a quality report.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Analyzing {url} ---\")\n",
        "\n",
        "    # 1. Fetch HTML\n",
        "    try:\n",
        "        # We MUST provide a User-Agent, or many sites will block us (403 Error)\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            return {\"error\": f\"Failed to fetch. Status code: {response.status_code}\"}\n",
        "\n",
        "        html_content = response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to fetch URL: {str(e)}\"}\n",
        "\n",
        "    # 2. Parse HTML (using our function from Phase 1)\n",
        "    # The 'parse_html' function must be defined in a previous cell!\n",
        "    try:\n",
        "        parsed_data = parse_html(html_content)\n",
        "    except NameError:\n",
        "        return {\"error\": \"The 'parse_html' function is not defined. Please run the Phase 1 cell.\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"HTML parsing failed: {str(e)}\"}\n",
        "\n",
        "    title = parsed_data['title']\n",
        "    body_text = parsed_data['body_text']\n",
        "    word_count = parsed_data['word_count']\n",
        "\n",
        "    if word_count == 0:\n",
        "        return {\"error\": \"Could not extract any body text from the page.\"}\n",
        "\n",
        "    # 3. Engineer Features\n",
        "    sentence_count = get_sentence_count(body_text) # (from Phase 2)\n",
        "    flesch_reading_ease = get_readability_score(body_text) # (from Phase 2)\n",
        "\n",
        "    # 4. Make Prediction\n",
        "    # We create a small DataFrame with the *exact* feature names\n",
        "    # the model was trained on. This stops the UserWarning.\n",
        "    feature_names = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
        "    features_df_live = pd.DataFrame(\n",
        "        [[word_count, sentence_count, flesch_reading_ease]],\n",
        "        columns=feature_names\n",
        "    )\n",
        "\n",
        "    quality_label = demo_model.predict(features_df_live)[0]\n",
        "    quality_probs = demo_model.predict_proba(features_df_live)[0]\n",
        "\n",
        "    # Create a nice dictionary of the probabilities\n",
        "    probabilities = {label: f\"{prob*100:.1f}%\" for label, prob in zip(demo_model.classes_, quality_probs)}\n",
        "\n",
        "    # 5. Format Output (as per assignment)\n",
        "    result = {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"metrics\": {\n",
        "            \"word_count\": word_count,\n",
        "            \"sentence_count\": sentence_count,\n",
        "            \"flesch_reading_ease\": round(flesch_reading_ease, 2)\n",
        "        },\n",
        "        \"quality_assessment\": {\n",
        "            \"predicted_label\": quality_label,\n",
        "            \"probabilities\": probabilities\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Return it as a nicely formatted JSON-like string\n",
        "    return json.dumps(result, indent=2)\n",
        "\n",
        "# === Let's test it! ======================================================\n",
        "# You can change this URL to any blog post\n",
        "test_url = \"https://www.varonis.com/blog/cybersecurity-tips\" # (One from our dataset)\n",
        "print(analyze_url(test_url))\n",
        "\n",
        "test_url_2 = \"https://neilpatel.com/blog/seo-copywriting/\" # (A random one)\n",
        "print(analyze_url(test_url_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJHGm1jlZsRh",
        "outputId": "0cde57ce-b03c-4939-b490-4c60afb3948e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded trained model 'models/quality_model.joblib'\n",
            "\n",
            "--- Analyzing https://www.varonis.com/blog/cybersecurity-tips ---\n",
            "{\n",
            "  \"url\": \"https://www.varonis.com/blog/cybersecurity-tips\",\n",
            "  \"title\": \"Top 10 Cybersecurity Awareness Tips: How to Stay Safe and Proactive\",\n",
            "  \"metrics\": {\n",
            "    \"word_count\": 1700,\n",
            "    \"sentence_count\": 92,\n",
            "    \"flesch_reading_ease\": 41.47\n",
            "  },\n",
            "  \"quality_assessment\": {\n",
            "    \"predicted_label\": \"Medium\",\n",
            "    \"probabilities\": {\n",
            "      \"High\": \"31.7%\",\n",
            "      \"Low\": \"12.2%\",\n",
            "      \"Medium\": \"56.1%\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "--- Analyzing https://neilpatel.com/blog/seo-copywriting/ ---\n",
            "{'error': 'Failed to fetch. Status code: 403'}\n"
          ]
        }
      ]
    }
  ]
}